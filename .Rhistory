data.prep.halfcantons = function(shift, response, startdate = "2020-07-06",
enddate = "2020-12-21", r.infovar) {
# shift: shifting of response when using casegrowth as defined
# response: in {casegrowth, median_R_mean}
# startdate, enddate: to be adjusted for different time periods
# r.infovar: lag length of information variables
# Output: data frame used for the estimation (Y, W, X) while merging the half cantons
# Note that the baseline mask policy of mandatory mask wearing was introduced nationally on "2020-07-06" and the
# first vaccinations took place on "2020-12-21". This is the reason for our sample selection.
# Read in data
DataCovid = read.csv(".\\Data\\DataCovid.csv")
dat.new = read.csv(".\\Data\\policy_stuff.csv", header = TRUE, sep = ",")
# Merge data on common days
data = merge(DataCovid, dat.new)
# Delete unnecessary variables to might lead to conflicts
data = data %>%
dplyr::select(-c(deaths_1d, recovered_1d, tests_1d, hosp_1d, value.total_1d, sre000d0_1d, tre200d0_1d, ure200d0_1d, O65P, restGatheringsCH)) %>%
rename(testingPolicy = testing_policyCH)
# Transform percentage to be a real percentage
data = data %>%
mutate(percentage_age = percentage_age*100)
# Get the weekly date as integer. Further construct disjoint weeks
data$oneweek = cut.Date(as.Date(data$datum), breaks = "1 week", start.on.monday = TRUE, labels = FALSE)
# Extract which value of oneweek corresponds to startdate, enddate to apply the time selection
# via the vector oneweek later
start.day = which(data$datum == startdate)[1]
start.week = data[start.day, "oneweek"]
end.day   = which(data$datum == enddate)[1]
end.week = data[end.day, "oneweek"]
# Make an indicator that treats normal cantons as separate cantons but half cantons as such (same value for the indicator)
data$geoRegion.2 = ifelse(data$geoRegion %in% c("BS", "BL"), "BSBL",
ifelse(data$geoRegion %in% c("AI", "AR"), "AIAR",
ifelse(data$geoRegion %in% c("OW", "NW"), "OWNW", data$geoRegion)))
if (response == "casegrowth") {
# Construct target variable and lagged target variable. We do that by computing the new cases per canton and week. We can then
# compute the log-growth rate using that series, which constructs 7 times that same growth rate per canton and week. When aggregating,
# we can then take the mean over disjoint weeks to obtain the desired variable. We do that for all growth rates.
# Construct national aggregates
sumTotalCH = aggregate(data$sumTotal, by = list(data$datum), FUN  = sum)
sumTotalCH = rename(sumTotalCH, datum = Group.1)
sumTotalCH = rename(sumTotalCH, sumTotalCH = x)
data = merge(sumTotalCH, data)
data = data %>%
arrange(geoRegion)
# Get new cases per day in each canton. Do it for the new cases on a national level as well.
data = data %>%
group_by(geoRegion) %>%
mutate(NewCases = c(0,diff(sumTotal)),
NewCases.national = c(0, diff(sumTotalCH))) %>%
ungroup()
data = data %>%
relocate(sumTotal, .before = NewCases)
# We now have all variables constant across oneweek and canton which need to be constant. The other ones
# are aggregated to weekly data by simply averaging the values per week and canton. Note that we take the sum
# for transactions and tests as we need the new number of transactions and tests per canton and week. These variables
# already come supplied as new transactions and tests so we do not have to treat them as the cases where we had to explicitly
# calculate the new cases as done in the above code.
data = data %>%
group_by(geoRegion.2, oneweek) %>%
summarize(newcases_we           = sum(NewCases),
newcases.national_we  = sum(NewCases.national),
newtransactions_we    = sum(Number.of.transactions),
newtests_we           = sum(tests),
percentage_age_we = mean(percentage_age),
Density_we        = mean(Density),
population_we     = mean(population),
grocery_and_pharmacy_we   = mean(grocery_and_pharmacy),
transit_stations_we       = mean(transit_stations),
workplaces_we             = mean(workplaces),
ferien_we                 = mean(ferien),
sre000d0_we               = mean(sre000d0),
tre200d0_we               = mean(tre200d0),
ure200d0_we               = mean(ure200d0),
schoolClosing_we          = mean(schoolClosing),
restGatherings_we         = mean(restGatherings),
cancEvents_we             = mean(cancEvents),
testingPolicy_we          = mean(testingPolicy),
workClosing2a_we          = mean(workClosing2a),
mask_treat_we             = mean(mask_treat)) %>%
ungroup()
# Using newcases_we, newcases.national_we, newtransactions_we and newtests_we, we can now compute the weekly growth rates.
# Create the response variable and lead response variable.
# Check if values are below the threshold of 0.1 and set it to 0.1 if so. Does not happen
data$newcases_we[data$newcases_we < 0.1] = 0.1
data$casegrowth_we = paneldiff(x = log(data$newcases_we), id = data$geoRegion.2, t = as.Date(data$oneweek), l = 1)
data$casegrowth_we = panellag(x = data$casegrowth_we, i = data$geoRegion.2, t = as.Date(data$oneweek), lag = -shift/7)
# Lagged target variable
data$casegrowthlag_we =  panellag(x = data$casegrowth_we, i = data$geoRegion.2, t = data$oneweek, lag = 2)
data$casegrowthlag1_we = panellag(x = data$casegrowth_we, i = data$geoRegion.2, t = data$oneweek, lag = 1)
# log Delta Cit
data$logdiffcases_we = lognozero(paneldiff(x = data$newcases_we, id = data$geoRegion.2, t = as.Date(data$oneweek), lag = 1))
# Lagged target variable on national level; works with id argument
data$newcases.national_we[data$newcases.national_we < 0.1] = 0.1
data$casegrowthlag.national_we = paneldiff(x = log(data$newcases.national_we), id = data$geoRegion.2, t = as.Date(data$oneweek), l = 1)
# log Delta Cit; same as above but national level
data$logdiffcases.national_we = lognozero(paneldiff(x = data$newcases.national_we, id = data$geoRegion.2, t = as.Date(data$oneweek), lag = 1))
# Delta log Tit; already new tests so we do not have to apply the difference operator
data$newtests_we[data$newtests_we < 0.1] = 0.1
data$difflogtests_we = paneldiff(x = log(data$newtests_we), id = data$geoRegion.2, t = as.Date(data$oneweek), l = 1)
# Delta log transactions
data$newtransactions_we[data$newtransactions_we < 0.1] = 0.1
data$transactiongrowth_we = paneldiff(x = log(data$newtransactions_we), id = data$geoRegion.2, t = as.Date(data$oneweek), l = 1)
# Correct period and name
data = data %>%
filter(oneweek >= start.week & oneweek <= end.week)
data = rename(data, Canton_3 = geoRegion.2)
# Construct X weekly
X = data[,c("casegrowthlag_we", "casegrowthlag1_we", "logdiffcases_we", "casegrowthlag.national_we", "logdiffcases.national_we", "difflogtests_we",
"percentage_age_we", "Density_we", "population_we",
"schoolClosing_we", "restGatherings_we", "cancEvents_we",
"testingPolicy_we", "workClosing2a_we",
"grocery_and_pharmacy_we", "transit_stations_we", "transactiongrowth_we", "workplaces_we",
"sre000d0_we", "tre200d0_we", "ure200d0_we", "ferien_we", "Canton_3", "oneweek")]
W = data$mask_treat_we
Y = data$casegrowth_we
}
if (response == "median_R_mean") {
# Take the mean over the disjoint weeks for (Y,W,X)
data = data %>%
group_by(geoRegion.2, oneweek) %>%
summarize(median_R_mean_we      = mean(median_R_mean),
percentage_age_we = mean(percentage_age),
Density_we        = mean(Density),
population_we     = mean(population),
grocery_and_pharmacy_we   = mean(grocery_and_pharmacy),
transit_stations_we       = mean(transit_stations),
newtransactions_we        = sum(Number.of.transactions),
workplaces_we             = mean(workplaces),
ferien_we                 = mean(ferien),
sre000d0_we               = mean(sre000d0),
tre200d0_we               = mean(tre200d0),
ure200d0_we               = mean(ure200d0),
schoolClosing_we          = mean(schoolClosing),
restGatherings_we         = mean(restGatherings),
cancEvents_we             = mean(cancEvents),
testingPolicy_we          = mean(testingPolicy),
workClosing2a_we          = mean(workClosing2a),
mask_treat_we             = mean(mask_treat)) %>%
ungroup()
# Delta log transactions
data$newtransactions_we[data$newtransactions_we < 0.1] = 0.1
data$transactiongrowth_we = paneldiff(x = log(data$newtransactions_we), id = data$geoRegion.2, t = as.Date(data$oneweek), l = 1)
# Lagged target variable
data$median_R_mean.lag_we = panellag(x = data$median_R_mean_we, i = data$geoRegion.2, t = data$oneweek, lag = r.infovar/7)
data$median_R_mean.lag1_we = panellag(x = data$median_R_mean_we, i = data$geoRegion.2, t = data$oneweek, lag = 1)
# Correct period and name
data = data %>%
filter(oneweek >= start.week & oneweek <= end.week)
data = rename(data, Canton_3 = geoRegion.2)
# Construct X weekly
X = data[,c("median_R_mean.lag_we", "median_R_mean.lag1_we",
"percentage_age_we", "Density_we", "population_we",
"schoolClosing_we", "restGatherings_we", "cancEvents_we",
"testingPolicy_we", "workClosing2a_we",
"grocery_and_pharmacy_we", "transit_stations_we", "transactiongrowth_we", "workplaces_we",
"sre000d0_we", "tre200d0_we", "ure200d0_we", "ferien_we", "Canton_3", "oneweek")]
W = data$mask_treat_we
Y = data$median_R_mean_we
}
# Return
return(list(data = data.frame(X = X, Y = Y, W = W)))
} # Data preparation half cantons
################################################################################
# (I) OVERVIEW
# This script runs all the proposed models using the helper function that are sourced
# from helperfunctions.R. Finally, the results are compiled such that they can be further
# called to create tables and plots.
################################################################################
# (II) ENVIRONMENT AND PACKAGES
# Empty environment
rm(list = ls())
# Reproducibility (bootstrap and more)
set.seed(42)
# Set working directory to the root node of folder structure
#setwd(".\\Mask_Project\\Final")
setwd("C:/Users/eminu/OneDrive/Desktop/Facial-Mask-Policy-COVID-19")
# Read helper functions. Note that they are in the same directory. Add the corresponding path
# otherwise.
#source(".\\Scripts\\helperfunctions.R")
source("./Code/helperfunctions.R")
################################################################################
# (III) ESTIMATION
# We will generate all models of interest. We start with the 2^3 models that we run
# through the estimation.R function, which are the fixed effects and random effects models. For case-growth,
# we further look at the results when including the additional information variables.
# We then run the 2^2 models that we run through the
# multiple_split.R function, which are the de-biased fixed effects models. Lastly, we estimate
# the 2^3 models using the double machine learning framework. We keep frequency in the loop for easy
# access to the results of the daily models if wanted.
# Start with the fixed effects and random effects models
freq.poss = c("weekly")
response.poss = c("median_R_mean", "casegrowth")
model.poss = c("within", "random")
#model.poss = c("within")
type.effect.poss = c("direct", "total")
# Allocation of results
results.list.fe = vector("list", 8)
i = 1
# Run the models
for (frequency in freq.poss) {
for (response in response.poss) {
for (model in model.poss) {
for (type.effect in type.effect.poss) {
results.list.fe[[i]] = list(results = estimation(frequency = frequency, response = response, model = model, infovar = FALSE, type.effect = type.effect),
frequency = frequency,
response = response,
model = model,
type.effect = type.effect)
i = i+1
}
}
}
}
# Run the models for response = case-growth and additional information variables. Note that we can only
# run it using the random effects approach as some of the additional information variables are at a national
# level which is not compatible with the fixed effects approach due to time fixed effects.
results.list.fe.infovar = vector("list", 2)
i = 1
# Run the models
for (frequency in freq.poss) {
for (type.effect in type.effect.poss) {
results.list.fe.infovar[[i]] = list(estimation(frequency = frequency, response = "casegrowth", model = "random", infovar = TRUE, type.effect = type.effect),
frequency = frequency,
response = "casegrowth",
model = "random",
type.effect = type.effect)
i = i+1
}
}
# Run the de-biased models via multiple sample splitting
results.list.dfe = vector("list", 4)
i = 1
# Run the models
for (frequency in freq.poss) {
for (response in response.poss) {
for(type.effect in type.effect.poss) {
results.list.dfe[[i]] = list(multiple_split(response = response, frequency = frequency, infovar = FALSE, type.effect = type.effect),
frequency = frequency,
response = response,
model = "debiased",
type.effect = type.effect)
i = i+1
}
}
}
# Additional, we run the double machine learning models using the lasso to estimate the nuisance functions
# and cluster-robust standard errors, which are clustered in two-way manner, meaning on time and cantons. For time,
# we use day for the daily model and weeks for the weekly model, just as in the fixed effects models.
results.list.dml = vector("list", 8)
i = 1
# Run the models
for (frequency in freq.poss) {
for (response in response.poss) {
for (infovar in c(TRUE, FALSE)) {
for (type.effect in type.effect.poss) {
results.list.dml[[i]] = list(dml.estim(response = response, frequency = frequency, infovar = infovar, type.effect = type.effect),
response = response,
frequency = frequency,
infovar = infovar,
type.effect = type.effect)
i = i+1
}
}
}
}
# Finally, we estimate cantonal-clustered multiplier bootstrapped standard errors for the fixed effects
# models.
results.list.bootstrap = vector("list", 4)
i = 1
# Run the bootstrap with B = 1000
# Run the models
for (frequency in freq.poss) {
for (response in response.poss) {
for (type.effect in type.effect.poss) {
results.list.bootstrap[[i]] = list(bs.estimation(frequency = frequency, response = response, infovar = FALSE, type.effect = type.effect),
frequency = frequency,
response = response,
type.effect = type.effect)
i = i+1
}
}
}
################################################################################
# (IV) STORE THE RESULTS (POINT ESTIMATES, STD. ERRORS, P-VALUES)
# Extract the results from the 5 lists that are differently structured. We do this in the order
# that they are run in the above code. Run the code in order (see variable j)
# Empty matrix with the corresponding dimensions
results.dat = matrix(NA, nrow = 102, ncol = 7)
colnames(results.dat) = c("estimate", "std.error", "p.val", "model", "term", "type.effect", "add.infovar")
# Model abbreviations from response and model type
abbrev = function(response, model) {
if (response == "median_R_mean" && model == "within") {
return("FE R")
} else if (response == "median_R_mean" && model == "random") {
return("RE R")
} else if (response == "casegrowth" && model == "within") {
return("FE Casegrowth")
} else if (response == "casegrowth" && model == "random") {
return("RE Casegrowth")
} else if (response == "median_R_mean" && model == "debiased") {
return("DFE R")
} else if (response == "casegrowth" && model == "debiased") {
return("DFE Casegrowth")
}
}
# Names of standard error estimators in correct ordering
cov.names = c("HC3", "Canton", "Time", "Canton-Time", "NW", "CH", "Own")
# Go through standard fixed effects and random effect estimation
j = 1
for (p in 1:8) {
for (k in 1:7) {
results.dat[j,1] = results.list.fe[[p]][["results"]][["results"]][[k]]["W", 1]
results.dat[j,2] = results.list.fe[[p]][["results"]][["results"]][[k]]["W", 2]
results.dat[j,3] = results.list.fe[[p]][["results"]][["results"]][[k]]["W", 4]
results.dat[j,4] = cov.names[k]
results.dat[j,5] = abbrev(response = results.list.fe[[p]][["response"]],
model    = results.list.fe[[p]][["model"]])
results.dat[j,6] = results.list.fe[[p]][["type.effect"]]
results.dat[j,7] = "FALSE"
j = j + 1
}
}
# Go through random effects models of case-growth with the additional information variables
# as in Chernozhukov (2020)
for (q in 1:2) {
results.dat[j,1] = results.list.fe.infovar[[q]][[1]][["results"]][[1]]["W", 1]
results.dat[j,2] = results.list.fe.infovar[[q]][[1]][["results"]][[1]]["W", 2]
results.dat[j,3] = results.list.fe.infovar[[q]][[1]][["results"]][[1]]["W", 4]
results.dat[j,4] = "HC3"
results.dat[j,5] = abbrev(response = results.list.fe.infovar[[q]][["response"]],
model    = results.list.fe.infovar[[q]][["model"]])
results.dat[j,6] = results.list.fe.infovar[[q]][["type.effect"]]
results.dat[j,7] = "TRUE"
j = j + 1
}
# Extract the results for the de-biased models. Match the specifications from the de-biased models
# to those of the non de-biased models.
for (d in 1:4) {
for (i in 1:7) {
results.dat[j,1] = results.list.dfe[[d]][[1]][["W"]]
results.dat[j,4] = cov.names[i]
results.dat[j,5] = abbrev(response = results.list.dfe[[d]][["response"]],
model    = results.list.dfe[[d]][["model"]])
results.dat[j,6] = results.list.dfe[[d]][["type.effect"]]
results.dat[j,7] = "FALSE"
j = j + 1
}
}
results.dat[(59:(59+6)),2] = results.dat[1:7,2]       # Std. errors from non de-biased model for FE R direct
results.dat[(66:(66+6)),2] = results.dat[8:(8+6),2]   # Std. errors from non de-biased model for FE R total
results.dat[(73:(73+6)),2] = results.dat[29:(29+6),2] # Std. errors from non de-biased model for FE Case-growth direct
results.dat[(80:(80+6)),2] = results.dat[36:(36+6),2] # Std. errors from non de-biased model for FE Case-growth direct
# Compute the p-values for the de-biased models via the t-distribution. Depending on the specification, we have a different
# number of degrees of freedom, i.e:
#      - FE Weekly direct: 589
#      - FE Weekly total: 590
#      - RE Weekly direct: 630
#      - RE Weekly total: 634
for (e in 59:86) {
# How many degrees of freedom
if (grepl("FE", results.dat[e,"term"]) == TRUE && results.dat[e,"type.effect"] == "direct") {
df = 589
} else if (grepl("FE", results.dat[e,"term"]) == TRUE && results.dat[e,"type.effect"] == "total") {
df = 590
} else if (grepl("RE", results.dat[e,"term"]) == TRUE && results.dat[e,"type.effect"] == "direct") {
df = 630
} else if (grepl("RE", results.dat[e,"term"]) == TRUE && results.dat[e,"type.effect"] == "total") {
df = 634
}
# Compute p value
results.dat[e,3] = 2*pt(-abs(as.numeric(results.dat[[e,1]])/as.numeric(results.dat[[e,2]])), df = df)
}
# Get the results from the double machine learning
for (l in 1:8) {
results.dat[j,1] = results.list.dml[[l]][[1]][1]
results.dat[j,2] = results.list.dml[[l]][[1]][2]
results.dat[j,3] = results.list.dml[[l]][[1]][4]
results.dat[j,4] = "DML"
if (results.list.dml[[l]][["response"]] == "median_R_mean") {
results.dat[j,5] = "DML R"
} else {
results.dat[j,5] = "DML Casegrowth"
}
results.dat[j,6] = results.list.dml[[l]][["type.effect"]]
results.dat[j,7] = results.list.dml[[l]][["infovar"]]
j = j + 1
}
# Get the results from the bootstrap
for (n in 1:4) {
results.dat[j,1] = results.list.bootstrap[[n]][[1]][1,"W"]
results.dat[j,2] = results.list.bootstrap[[n]][[1]][2,"W"]
if (results.list.bootstrap[[n]][["type.effect"]] == "direct") {
df = 589
} else {
df = 590
}
results.dat[j,3] = 2*pt(-abs(results.list.bootstrap[[n]][[1]][1,"W"]/results.list.bootstrap[[n]][[1]][2,"W"]), df = df)
results.dat[j,4] = "Canton-Bootstrap"
results.dat[j,5] = abbrev(response = results.list.bootstrap[[n]][["response"]], model = "within")
results.dat[j,6] = results.list.bootstrap[[n]][["type.effect"]]
results.dat[j,7] = "FALSE"
j = j + 1
}
# Bootstrapped standard errors for de-biased models. Make an array containing a
# triple in (row) x (row) x (degree of freedom) x (term) x (type.effect)-space to loop over.
v1 = c(59, 95, 589, "DFE R", "direct") # DFE R direct
v2 = c(66, 96, 590, "DFE R", "total")  # DFE R total
v3 = c(73, 97, 589, "DFE Casegrowth", "direct") # DFE case growth direct
v4 = c(80, 98, 590, "DFE Casegrowth", "total") # DFE case growth total
arr = array(c(v1, v2, v3, v4), dim = c(5,4))
for (g in 1:4) {
results.dat[j,1] = as.numeric(results.dat[as.numeric(arr[1,g]),1])
results.dat[j,2] = as.numeric(results.dat[as.numeric(arr[2,g]),2])
results.dat[j,3] = 2*pt(-abs(as.numeric(results.dat[j,1])/as.numeric(results.dat[j,2])), df = as.numeric(arr[3,g]))
results.dat[j,4] = "Canton-Bootstrap"
results.dat[j,5] = arr[4,g]
results.dat[j,6] = arr[5,g]
results.dat[j,7] = FALSE
j = j + 1
}
# Save as data frame and export it as csv file
results.dat = as.data.frame(results.dat)
# Remove rows with non-HC3 estimators for the random effects model as they are not actually estimated.
results.dat = results.dat %>%
filter(!(model != "HC3" & term == "RE R")) %>%
filter(!(model != "HC3" & term == "RE Casegrowth"))
# Write csv
write.csv(results.dat,"./Data/results_short_period.csv", row.names = FALSE)
# (I) OVERVIEW
# This script produces the plots from the paper.
################################################################################
# (II) ENVIRONMENT AND PACKAGES
# Empty environment
rm(list = ls())
# Set working directory to the root node of folder structure
#setwd(".\\Mask_Project\\Final")
setwd("C:/Users/eminu/OneDrive/Desktop/Facial-Mask-Policy-COVID-19")
# Read helper functions. Note that they are in the same directory. Add the corresponding path
# otherwise.
#source(".\\Scripts\\helperfunctions.R")
source("./Code/helperfunctions.R")
# # Read in fonts
# font_install("fontcm")
# loadfonts()
################################################################################
# (III) PLOTS
# (i) CI Plots
# Read in data; change col-names to correspond to dw-plot; make ordering consistent; remove those with additional
# information variables as well as the DML rows.
results = read.csv(".\\Data\\results_short_period.csv", header = T, sep = ",", stringsAsFactors = FALSE)
results = results %>%
filter(!model == "DML") %>%
filter(!model == "Canton-Bootstrap") %>%
filter(add.infovar == FALSE)
results[results == "FE R"] = "FE r"
results[results == "DFE R"] = "DFE r"
results[results == "RE R"] = "RE r"
results[results == "FE Casegrowth"] = "FE growth.new.cases"
results[results == "DFE Casegrowth"] = "DFE growth.new.cases"
results[results == "RE Casegrowth"] = "RE growth.new.cases"
results[results == "Canton-Time"] = "Canton-Week"
results[results == "Time"] = "Week"
results.direct = results %>%
filter(type.effect == "direct")
results.total = results %>%
filter(type.effect == "total")
# Direct effect
pdf(".\\Plots\\ci_plot_final_direct_short.pdf", width = 10, height = 10*1.414)
dwplot(results.direct, conf.level = 0.95, dodge_size = 0.6,
vars_order = c("FE r", "FE growth.new.cases",
"DFE r", "DFE growth.new.cases",
"RE r", "RE growth.new.cases"),
whisker_args = list(size = 1.5),
dot_args = list(size = 3.5, shape = 20)) +
theme_bw(base_line_size = 1.5) +
scale_color_brewer(palette = "Dark2",
name = "Standard Errors",
guide = guide_legend(override.aes = list(linetype = 1,
shape = NA))) +
theme(text = element_text(color = "black", size = 14),
axis.text = element_text(color = "black", size = 14),
axis.text.x = element_text(color = "black", size = 14),
legend.text = element_text(color = "black", size = 14),
panel.grid.major.y = element_blank())+
xlab("Estimated direct effect of mask policy and corresponding 95%-CI") +
scale_x_continuous(breaks = c(0.1,0.0,-0.1,-0.2,-0.3,-0.4,-0.5,-0.6,-0.7), limits = c(-0.7, 0.08),
minor_breaks = c(0.05,-0.05,-0.15,-0.25,-0.35,-0.45,-0.55,-0.65)) +
geom_vline(xintercept = 0, colour = "grey60", linetype = 2)
dev.off()
# Total effect
pdf(".\\Plots\\ci_plot_final_total_short.pdf", width = 10, height = 10*1.414)
dwplot(results.total, conf.level = 0.95, dodge_size = 0.6,
vars_order = c("FE r", "FE growth.new.cases",
"DFE r", "DFE growth.new.cases",
"RE r", "RE growth.new.cases"),
whisker_args = list(size = 1.5),
dot_args = list(size = 3.5, shape = 20)) +
theme_bw(base_line_size = 1.5) +
scale_color_brewer(palette = "Dark2",
name = "Standard Errors",
guide = guide_legend(override.aes = list(linetype = 1,
shape = NA))) +
theme(text = element_text(color = "black", size = 14),
axis.text = element_text(color = "black", size = 14),
axis.text.x = element_text(color = "black", size = 14),
legend.text = element_text(color = "black", size = 14),
panel.grid.major.y = element_blank()) +
xlab("Estimated total effect of mask policy and corresponding 95%-CI") +
scale_x_continuous(breaks = c(0.1,0.0,-0.1,-0.2,-0.3,-0.4,-0.5,-0.6,-0.7), limits = c(-0.7, 0.08),
minor_breaks = c(0.05,-0.05,-0.15,-0.25,-0.35,-0.45,-0.55,-0.65)) +
geom_vline(xintercept = 0, colour = "grey60", linetype = 2)
dev.off()
